{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import ast\n",
    "import numpy as np\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9988259673118591}]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline('sentiment-analysis')('we love you'))\n",
    "print(pipeline('sentiment-analysis')('we hate you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roberta_neg': 0.0018993656, 'roberta_neu': 0.013890146, 'roberta_pos': 0.9842106}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer('very good movie!', return_tensors='pt')\n",
    "output = model(**encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores_dict = {\n",
    "    'roberta_neg' : scores[0],\n",
    "    'roberta_neu' : scores[1],\n",
    "    'roberta_pos' : scores[2]\n",
    "}\n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    try:\n",
    "        encoded_text = tokenizer(example, return_tensors='pt')\n",
    "        output = model(**encoded_text)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        scores_dict = {\n",
    "            'roberta_neg' : scores[0],\n",
    "            'roberta_neu' : scores[1],\n",
    "            'roberta_pos' : scores[2]\n",
    "        }\n",
    "    except:\n",
    "        scores_dict = {\n",
    "            'roberta_neg' : 0,\n",
    "            'roberta_neu' : 0,\n",
    "            'roberta_pos' : 0\n",
    "        }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_return_file(folder_path):\n",
    "    files_path = os.listdir(folder_path)\n",
    "    files_path.sort()\n",
    "    data_path_daily = ['{}{}'.format(folder_path,file) for file in files_path]\n",
    "    data_path_daily = pd.DataFrame(data_path_daily,columns=['File Path'])\n",
    "    data_path_daily['File Name'] = files_path\n",
    "    return data_path_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19966/19966 [32:58<00:00, 10.09it/s]\n",
      "100%|██████████| 20128/20128 [31:36<00:00, 10.61it/s]\n",
      "100%|██████████| 20582/20582 [32:39<00:00, 10.51it/s]\n",
      "100%|██████████| 20409/20409 [32:51<00:00, 10.35it/s]\n",
      "100%|██████████| 20406/20406 [33:52<00:00, 10.04it/s]\n",
      "5it [2:44:03, 1968.65s/it]\n"
     ]
    }
   ],
   "source": [
    "for i,file in tqdm(enumerate(read_return_file('Datasets/Processed Tweet/'))):\n",
    "    try:\n",
    "        data = pd.read_csv(file,engine='python',index_col=0)\n",
    "        data['sent_score']= data['clean_tweet'].astype('str').progress_apply(polarity_scores_roberta)\n",
    "        data.to_csv('{}'.format(file))\n",
    "    except:\n",
    "        print(i)\n",
    "        print(file)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:16,  2.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for i,file in tqdm(enumerate(read_return_file('Datasets/Processed Tweet/'))):\n",
    "    data = pd.read_csv(file,engine='python',index_col=0)\n",
    "    if 'POS' not in data.columns:\n",
    "        try:\n",
    "            data['sent_score'] = data['sent_score'].apply(lambda x: ast.literal_eval(x))\n",
    "            data['POS'] = data['sent_score'].apply(lambda x: x['roberta_pos'])\n",
    "            data['NEU'] = data['sent_score'].apply(lambda x: x['roberta_neu'])\n",
    "            data['NEG'] = data['sent_score'].apply(lambda x: x['roberta_neg'])\n",
    "            data.to_csv('{}'.format(file))\n",
    "        except:\n",
    "            print(i)\n",
    "            print(file)\n",
    "            continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['following','followers','totaltweets','retweetcount','created_at', 'clean_tweet','POS','NEU','NEG']\n",
    "for i,file in read_return_file('Datasets/Processed Tweet/').iterrows():\n",
    "    data = pd.read_csv(file['File Path'],engine='python',index_col=0)\n",
    "    data = data[col_names]\n",
    "    data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "    data.to_csv('Datasets/Reduced Tweet Data/{}'.format(file['File Name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:25,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#Average Sentiment Score Daily\n",
    "sentiment_scores = pd.DataFrame(columns=['POS','NEU','NEG'])\n",
    "for i, file in tqdm(read_return_file('Datasets/Reduced Tweet Data/').iterrows()):\n",
    "    data = pd.read_csv(file['File Path'],engine='python',index_col=0)\n",
    "    data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "    #daily_score = pd.DataFrame(data.groupby([data['created_at'].dt.hour])[['POS','NEU','NEG']].mean().mean())\n",
    "    daily_score = pd.DataFrame(data[['POS','NEU','NEG']].mean())\n",
    "    daily_score = daily_score.transpose()\n",
    "    sentiment_scores = pd.concat([sentiment_scores,daily_score],axis=0,ignore_index=True)\n",
    "\n",
    "sentiment_scores.to_csv('Datasets/sentiment_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers = data['followers']\n",
    "followers = pd.DataFrame(followers)\n",
    "followers['scale'] = scale(followers['followers'])\n",
    "scale_data = followers['scale']\n",
    "scale_data = np.array(scale_data)\n",
    "scale_data = scale_data.reshape(-1,1)\n",
    "clustering = KMeans(n_clusters=4,random_state=5)\n",
    "clustering.fit(scale_data)\n",
    "followers['labels'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = plt.figure()\n",
    "# f.set_figwidth(10)\n",
    "# f.set_figheight(5)\n",
    "# cdict = {0:'yellow',1: 'red', 2: 'blue', 3: 'green'}\n",
    "\n",
    "# scatter_x = np.array(followers.index)\n",
    "# scatter_y = np.array(followers['followers'])\n",
    "# group = np.array(followers['labels'])\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize =(10,20))\n",
    "# fig.set_figwidth = 200\n",
    "# for g in np.unique(group):\n",
    "#     ix =np.where(group==g)\n",
    "#     ax.scatter(scatter_x[ix],scatter_y[ix],c=cdict[g],label=g,s = 100)\n",
    "#     fig.set_figwidth = 200\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:32,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, file in tqdm(read_return_file('Datasets/Reduced Tweet Data/').iterrows()):\n",
    "    try:\n",
    "        data = pd.read_csv(file['File Path'],engine='python',index_col=0)\n",
    "        data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "        followers = data['followers']\n",
    "        followers = pd.DataFrame(followers)\n",
    "        followers['scale'] = scale(followers['followers'])\n",
    "        scale_data = followers['scale']\n",
    "        scale_data = np.array(scale_data)\n",
    "        scale_data = scale_data.reshape(-1,1)\n",
    "        clustering = KMeans(n_clusters=50,random_state=5)\n",
    "        clustering.fit(scale_data)\n",
    "        data['influence_labels'] = clustering.labels_\n",
    "        data.to_csv('Datasets/Reduced Tweet Data/{}'.format(file['File Name']))\n",
    "    except:\n",
    "        print(file['File Name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Sentiment Score Daily of \"Influences\"\n",
    "\n",
    "sentiment_scores_influencer = pd.DataFrame(columns=['POS','NEU','NEG'])\n",
    "for i, file in read_return_file('Datasets/Reduced Tweet Data/').iterrows():\n",
    "    data = pd.read_csv(file['File Path'],engine='python',index_col=0)\n",
    "    #print(data['created_at'][0],file['File Name'])\n",
    "    influence_count = data['influence_labels'].value_counts()\n",
    "    data = data[data['influence_labels']!=influence_count.idxmax()]\n",
    "    daily_score = pd.DataFrame(data[['POS','NEU','NEG']].mean())\n",
    "    daily_score = daily_score.transpose()\n",
    "    sentiment_scores_influencer = pd.concat([sentiment_scores_influencer,daily_score],axis=0,ignore_index=True)\n",
    "\n",
    "sentiment_scores_influencer.to_csv('Datasets/sentiment_scores_influencer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot average sent score total\n",
    "#Plot average sent score\n",
    "#Plot influencer sent score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch_tf_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3525862eb7474a29db09ce01a834f42964e4c1bfc31ad17c337acfcc423c297f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
