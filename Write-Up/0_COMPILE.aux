\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{List of Figures}{ii}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{List of Tables}{iii}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgement}{iv}{chapter*.4}\protected@file@percent }
\citation{*}
\bibstyle{agsm}
\@writefile{toc}{\contentsline {section}{Abstract}{v}{chapter*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Terms and Definitions}{vi}{chapter*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{McCulloch:1943aa}
\citation{Rosenblatt:1958aa}
\citation{Rumelhart:1986aa}
\citation{LeCun:2015aa}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Learning Networks}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{McCulloch:1943aa}
\citation{Rosenblatt:1958aa}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Perceptron}{3}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Rosenblatt perceptron - TBC\relax }}{4}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{rosenblatt_perceptron}{{2.1}{4}{Rosenblatt perceptron - TBC\relax }{figure.caption.7}{}}
\newlabel{eq:mcp_neuron}{{2.1}{4}{The Perceptron}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Set $\mathbf  {A}$ and $\mathbf  {B}$ with a random separating line - TBC\relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:perceptron_example_2}{{2.2}{5}{Set $\mathbf {A}$ and $\mathbf {B}$ with a random separating line - TBC\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:perceptron_example_3}{{2.3}{6}{!!!TO BE CHANGED!!!\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces !!!!TO BE CHANGED!!!!\relax }}{6}{figure.caption.10}\protected@file@percent }
\newlabel{fig:perceptron_example_4}{{2.4}{6}{!!!!TO BE CHANGED!!!!\relax }{figure.caption.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Perceptron Algorithm\relax }}{7}{algorithm.1}\protected@file@percent }
\newlabel{alg:perceptron_algorithm}{{1}{7}{Perceptron Algorithm\relax }{algorithm.1}{}}
\newlabel{eq:per_cov_ine_1}{{2.8}{8}{The Perceptron}{equation.2.1.8}{}}
\newlabel{eq:per_cov_ine_2}{{2.9}{8}{The Perceptron}{equation.2.1.9}{}}
\newlabel{eq:per_cov_ine_3}{{2.10}{9}{The Perceptron}{equation.2.1.10}{}}
\newlabel{eq:induction_1}{{2.11}{9}{The Perceptron}{equation.2.1.11}{}}
\newlabel{eq:induction_2}{{2.12}{9}{The Perceptron}{equation.2.1.12}{}}
\newlabel{eq:induction_3}{{2.13}{9}{The Perceptron}{equation.2.1.13}{}}
\newlabel{eq:induction_starts}{{2.14}{9}{The Perceptron}{equation.2.1.14}{}}
\newlabel{eq:perp_cov_result1}{{2.15}{9}{The Perceptron}{equation.2.1.15}{}}
\newlabel{eq:induction_proof_part2}{{2.16}{9}{The Perceptron}{equation.2.1.16}{}}
\newlabel{eq:state1_conditions}{{2.17}{10}{The Perceptron}{equation.2.1.17}{}}
\newlabel{eq:state_1_proof}{{2.18}{10}{The Perceptron}{equation.2.1.18}{}}
\newlabel{eq:state2_conditions}{{2.20}{10}{The Perceptron}{equation.2.1.20}{}}
\newlabel{eq:state_2_proof}{{2.21}{10}{The Perceptron}{equation.2.1.21}{}}
\newlabel{eq:perp_cov_result2}{{2.23}{11}{The Perceptron}{equation.2.1.23}{}}
\newlabel{eq:CS_inquality}{{2.24}{11}{The Perceptron}{equation.2.1.24}{}}
\newlabel{eq:after_cs}{{2.25}{11}{The Perceptron}{equation.2.1.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces AND Function with separating line\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{AND_function_2}{{2.5}{12}{AND Function with separating line\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces AND Function with separating line\relax }}{13}{figure.caption.14}\protected@file@percent }
\newlabel{OR_function}{{2.6}{13}{AND Function with separating line\relax }{figure.caption.14}{}}
\newlabel{eq:XOR_function}{{2.35}{15}{XOR Function}{equation.2.1.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces OR function\relax }}{15}{figure.caption.16}\protected@file@percent }
\newlabel{XOR_function}{{2.7}{15}{OR function\relax }{figure.caption.16}{}}
\newlabel{eq:error_XOR_prob}{{2.36}{15}{XOR Function}{equation.2.1.36}{}}
\newlabel{eq:bias_derivative}{{2.37}{15}{XOR Function}{equation.2.1.37}{}}
\newlabel{eq:weight_derivative}{{2.38}{16}{XOR Function}{equation.2.1.38}{}}
\newlabel{eq:first_set_normal_eq}{{2.42}{16}{XOR Function}{equation.2.1.42}{}}
\newlabel{eq:second_set_normal_eq}{{2.46}{17}{XOR Function}{equation.2.1.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces XOR approximate solution\relax }}{18}{figure.caption.17}\protected@file@percent }
\newlabel{fig: approximate_XOR_solution}{{2.8}{18}{XOR approximate solution\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Feedforward Network}{18}{section.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Truth Table for $f_1^{*}$\relax }}{19}{table.caption.18}\protected@file@percent }
\newlabel{table:truth_table_f_1}{{2.1}{19}{Truth Table for $f_1^{*}$\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces $f_1$\relax }}{19}{figure.caption.19}\protected@file@percent }
\newlabel{fig:f1_XOR}{{2.9}{19}{$f_1$\relax }{figure.caption.19}{}}
\newlabel{eq: weight_f_1}{{2.50}{19}{Feedforward Network}{equation.2.2.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Truth Table for $f_2^{*}$\relax }}{20}{table.caption.20}\protected@file@percent }
\newlabel{table:truth_table_f_2}{{2.2}{20}{Truth Table for $f_2^{*}$\relax }{table.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces $f_1$\relax }}{20}{figure.caption.21}\protected@file@percent }
\newlabel{fig:f2_XOR}{{2.10}{20}{$f_1$\relax }{figure.caption.21}{}}
\newlabel{eq: weight_f_2}{{2.52}{20}{Feedforward Network}{equation.2.2.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Truth Table for $f^{*}$\relax }}{21}{table.caption.22}\protected@file@percent }
\newlabel{table:truth_table_f*}{{2.3}{21}{Truth Table for $f^{*}$\relax }{table.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces FFN hidden layer\relax }}{21}{figure.caption.23}\protected@file@percent }
\newlabel{fig:hidden_layer_only}{{2.11}{21}{FFN hidden layer\relax }{figure.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Truth Table from hidden layer\relax }}{22}{table.caption.24}\protected@file@percent }
\newlabel{table:truth_table_hidden_layer}{{2.4}{22}{Truth Table from hidden layer\relax }{table.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces XOR layer\relax }}{22}{figure.caption.25}\protected@file@percent }
\newlabel{fig:XOR layer}{{2.12}{22}{XOR layer\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Feedforward network for XOR problem\relax }}{23}{figure.caption.26}\protected@file@percent }
\newlabel{fig: ffn_XOR_final}{{2.13}{23}{Feedforward network for XOR problem\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Function}{25}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Backpropagation}{27}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{27}{figure.caption.27}\protected@file@percent }
\newlabel{fig:bp_nn_complete}{{2.14}{27}{!!!TO BE CHANGED!!!\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{28}{figure.caption.28}\protected@file@percent }
\newlabel{fig:bp_nn_complete_cons_layer}{{2.15}{28}{!!!TO BE CHANGED!!!\relax }{figure.caption.28}{}}
\newlabel{eq:activation_function_at_l+1 layer}{{2.65}{28}{Backpropagation}{equation.2.4.65}{}}
\newlabel{eq:input of activation_function_at_l+1 layer}{{2.66}{28}{Backpropagation}{equation.2.4.66}{}}
\newlabel{eq:cost_function_definition}{{2.67}{29}{Backpropagation}{equation.2.4.67}{}}
\newlabel{eq:BP_EQ_VEC_1}{{2.68}{29}{Backpropagation}{equation.2.4.68}{}}
\newlabel{eq: change_at_layer_l}{{2.69}{30}{Backpropagation}{equation.2.4.69}{}}
\newlabel{eq: delta_of_cost_bet_layers}{{2.70}{30}{Backpropagation}{equation.2.4.70}{}}
\newlabel{eq: z_j partials}{{2.71}{30}{Backpropagation}{equation.2.4.71}{}}
\newlabel{eq:BP_EQ_VEC_2}{{2.72}{30}{Backpropagation}{equation.2.4.72}{}}
\newlabel{eq:BP_EQ_VEC_3}{{2.73}{31}{Backpropagation}{equation.2.4.73}{}}
\newlabel{eq: partial_C_with_weight}{{2.74}{31}{Backpropagation}{equation.2.4.74}{}}
\newlabel{eq: partial_act_weight}{{2.75}{31}{Backpropagation}{equation.2.4.75}{}}
\newlabel{eq:BP_EQ_VEC_4}{{2.76}{32}{Backpropagation}{equation.2.4.76}{}}
\citation{Rumelhart:1986aa}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Back-Propagation Algorithm\relax }}{33}{algorithm.2}\protected@file@percent }
\newlabel{alg:back_propagation_algo}{{2}{33}{Back-Propagation Algorithm\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Sequential Neural Networks}{33}{section.2.5}\protected@file@percent }
\newlabel{eq:RNN_HL}{{2.85}{33}{Recurrent Neural Network(RNN)}{equation.2.5.85}{}}
\citation{Bengio1994}
\citation{Kolen2001}
\citation{Hochreiter1997}
\newlabel{eq:RNN_OL}{{2.86}{34}{Recurrent Neural Network(RNN)}{equation.2.5.86}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{34}{figure.caption.30}\protected@file@percent }
\newlabel{}{{2.16}{34}{!!!TO BE CHANGED!!!\relax }{figure.caption.30}{}}
\newlabel{eq:LSTM_HS}{{2.87}{35}{Long Short Term Memory(LSTM)}{equation.2.5.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{35}{figure.caption.32}\protected@file@percent }
\newlabel{LSTM_LAYER}{{2.17}{35}{!!!TO BE CHANGED!!!\relax }{figure.caption.32}{}}
\citation{Schurster1997}
\newlabel{eq:BRNN_LAYER}{{2.91}{36}{Bidirectional RNN(BRNN)}{equation.2.5.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{37}{figure.caption.34}\protected@file@percent }
\newlabel{BRNN_LAYER}{{2.18}{37}{!!!TO BE CHANGED!!!\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Optimization}{38}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:optimization_problem}{{3.1}{38}{Optimization}{equation.3.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces !!!TO BE CHANGED!!! Min max \relax }}{39}{figure.caption.36}\protected@file@percent }
\newlabel{fig:min_max_illustration}{{3.1}{39}{!!!TO BE CHANGED!!! Min max \relax }{figure.caption.36}{}}
\newlabel{eq:grad_f_x}{{3.2}{39}{Determining Minimum/Maximum Point}{equation.3.0.2}{}}
\newlabel{eq:Hessian_matrix}{{3.3}{39}{Determining Minimum/Maximum Point}{equation.3.0.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Optimization algorithms}{40}{section.3.1}\protected@file@percent }
\newlabel{network_error}{{3.4}{40}{Optimization algorithms}{equation.3.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Network Error\relax }}{40}{figure.caption.38}\protected@file@percent }
\newlabel{network_error_graph}{{3.2}{40}{Network Error\relax }{figure.caption.38}{}}
\newlabel{derivative_network_error}{{3.5}{41}{Gradient Descent}{equation.3.1.5}{}}
\newlabel{eq:SGM_def}{{3.8}{42}{Stochastic Gradient Descent}{equation.3.1.8}{}}
\newlabel{eq: learning_rates}{{3.9}{42}{Stochastic Gradient Descent}{equation.3.1.9}{}}
\newlabel{mb-sgd}{{3.12}{43}{Mini-Batch Stochastic Gradient Descent}{equation.3.1.12}{}}
\newlabel{eq:ewma_def}{{3.18}{44}{Exponentially Weighted Moving Average}{equation.3.1.18}{}}
\newlabel{eq: SGF_with_momentum}{{3.19}{44}{SGD with Momentum}{equation.3.1.19}{}}
\newlabel{eq:RMSProp_def}{{3.20}{45}{RMSProp}{equation.3.1.20}{}}
\newlabel{eq:Adam_def}{{3.21}{46}{Adam}{equation.3.1.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation using Adam}{46}{section.3.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Backpropagation using Adam optimization with $n$ total number of inputs for $N$ epochs\relax }}{47}{algorithm.3}\protected@file@percent }
\newlabel{alg:back_propagation_adam_algo}{{3}{47}{Backpropagation using Adam optimization with $n$ total number of inputs for $N$ epochs\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Sentiment Analysis}{48}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Rule-Based Methods}{49}{section.4.1}\protected@file@percent }
\citation{Vaswani2017}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Word Embedding}{51}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Transformer Model (Attention Mechanism)}{52}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces !!!TO BE CHANGED!!!-Transformer Layer\relax }}{52}{figure.caption.49}\protected@file@percent }
\newlabel{TRANSFORMER_LAYER}{{4.1}{52}{!!!TO BE CHANGED!!!-Transformer Layer\relax }{figure.caption.49}{}}
\citation{Jacob2018}
\citation{Vaswani2017}
\citation{Chiorrini2021}
\citation{Yinhan2019}
\citation{Yinhan2019}
\bibdata{REFERENCES}
\harvardcite{Bengio1994}{Bengio, Simard \harvardand \^^MFrasconi}{Bengio et~al.}{1994}
\harvardcite{Chiorrini2021}{Chiorrini, Diamantini, Mircoli \harvardand \^^MPotena}{Chiorrini et~al.}{2021}
\harvardcite{Jacob2018}{Devlin, Chang, Lee \harvardand \^^MToutanova}{Devlin et~al.}{2018}
\harvardcite{Hochreiter1997}{Hochreiter \harvardand \ Schmidhuber}{Hochreiter \harvardand \ Schmidhuber}{1997}
\harvardcite{Kolen2001}{Kolen \harvardand \ Kremer}{Kolen \harvardand \ Kremer}{2001}
\harvardcite{LeCun:2015aa}{LeCun, Bengio \harvardand \^^MHinton}{LeCun et~al.}{2015}
\harvardcite{Yinhan2019}{Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer \harvardand \ Stoyanov}{Liu et~al.}{2019}
\harvardcite{McCulloch:1943aa}{McCulloch \harvardand \ Pitts}{McCulloch \harvardand \ Pitts}{1943}
\harvardcite{Rosenblatt:1958aa}{Rosenblatt}{Rosenblatt}{1958}
\harvardcite{Rumelhart:1986aa}{Rumelhart, Hinton \harvardand \^^MWilliams}{Rumelhart et~al.}{1986}
\harvardcite{Schurster1997}{Schuster \harvardand \ Paliwal}{Schuster \harvardand \ Paliwal}{1997}
\harvardcite{Vaswani2017}{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser \harvardand \ Polosukhin}{Vaswani et~al.}{2017}
\gdef \@abspage@last{62}
