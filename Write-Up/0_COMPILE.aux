\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{List of Figures}{ii}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{List of Tables}{iii}{chapter*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Acknowledgement}{iv}{chapter*.4}\protected@file@percent }
\citation{*}
\bibstyle{agsm}
\@writefile{toc}{\contentsline {section}{Abstract}{v}{chapter*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Terms and Definitions}{vi}{chapter*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{McCulloch:1943aa}
\citation{Rosenblatt:1958aa}
\citation{Rumelhart:1986aa}
\citation{LeCun:2015aa}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Learning Networks}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{McCulloch:1943aa}
\citation{Rosenblatt:1958aa}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Perceptron}{3}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Rosenblatt perceptron - TBC\relax }}{4}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{rosenblatt_perceptron}{{2.1}{4}{Rosenblatt perceptron - TBC\relax }{figure.caption.7}{}}
\newlabel{eq:mcp_neuron}{{2.1}{4}{The Perceptron}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Set $\mathbf  {A}$ and $\mathbf  {B}$ with a random separating line\relax }}{5}{figure.caption.8}\protected@file@percent }
\newlabel{fig:perceptron_example_2}{{2.2}{5}{Set $\mathbf {A}$ and $\mathbf {B}$ with a random separating line\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:perceptron_example_3}{{2.3}{6}{!!!TO BE CHANGED!!!\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces !!!!TO BE CHANGED!!!!\relax }}{6}{figure.caption.10}\protected@file@percent }
\newlabel{fig:perceptron_example_4}{{2.4}{6}{!!!!TO BE CHANGED!!!!\relax }{figure.caption.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Perceptron Algorithm\relax }}{7}{algorithm.1}\protected@file@percent }
\newlabel{alg:perceptron_algorithm}{{1}{7}{Perceptron Algorithm\relax }{algorithm.1}{}}
\newlabel{eq:per_cov_ine_1}{{2.8}{8}{The Perceptron}{equation.2.1.8}{}}
\newlabel{eq:per_cov_ine_2}{{2.9}{8}{The Perceptron}{equation.2.1.9}{}}
\newlabel{eq:per_cov_ine_3}{{2.10}{9}{The Perceptron}{equation.2.1.10}{}}
\newlabel{eq:induction_1}{{2.11}{9}{The Perceptron}{equation.2.1.11}{}}
\newlabel{eq:induction_2}{{2.12}{9}{The Perceptron}{equation.2.1.12}{}}
\newlabel{eq:induction_3}{{2.13}{9}{The Perceptron}{equation.2.1.13}{}}
\newlabel{eq:induction_starts}{{2.14}{9}{The Perceptron}{equation.2.1.14}{}}
\newlabel{eq:perp_cov_result1}{{2.15}{9}{The Perceptron}{equation.2.1.15}{}}
\newlabel{eq:induction_proof_part2}{{2.16}{9}{The Perceptron}{equation.2.1.16}{}}
\newlabel{eq:state1_conditions}{{2.17}{10}{The Perceptron}{equation.2.1.17}{}}
\newlabel{eq:state_1_proof}{{2.18}{10}{The Perceptron}{equation.2.1.18}{}}
\newlabel{eq:state2_conditions}{{2.20}{10}{The Perceptron}{equation.2.1.20}{}}
\newlabel{eq:state_2_proof}{{2.21}{10}{The Perceptron}{equation.2.1.21}{}}
\newlabel{eq:perp_cov_result2}{{2.23}{11}{The Perceptron}{equation.2.1.23}{}}
\newlabel{eq:CS_inquality}{{2.24}{11}{The Perceptron}{equation.2.1.24}{}}
\newlabel{eq:after_cs}{{2.25}{11}{The Perceptron}{equation.2.1.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces AND Function with separating line\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{AND_function_2}{{2.5}{12}{AND Function with separating line\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces OR Function with separating line\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{OR_function}{{2.6}{14}{OR Function with separating line\relax }{figure.caption.14}{}}
\newlabel{eq:XOR_function}{{2.35}{15}{XOR Function}{equation.2.1.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces XOR function\relax }}{15}{figure.caption.16}\protected@file@percent }
\newlabel{XOR_function}{{2.7}{15}{XOR function\relax }{figure.caption.16}{}}
\newlabel{eq:error_XOR_prob}{{2.36}{16}{XOR Function}{equation.2.1.36}{}}
\newlabel{eq:bias_derivative}{{2.37}{16}{XOR Function}{equation.2.1.37}{}}
\newlabel{eq:weight_derivative}{{2.38}{16}{XOR Function}{equation.2.1.38}{}}
\newlabel{eq:first_set_normal_eq}{{2.42}{17}{XOR Function}{equation.2.1.42}{}}
\newlabel{eq:second_set_normal_eq}{{2.47}{18}{XOR Function}{equation.2.1.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces XOR approximate solution\relax }}{19}{figure.caption.17}\protected@file@percent }
\newlabel{fig: approximate_XOR_solution}{{2.8}{19}{XOR approximate solution\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Feedforward Network}{19}{section.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Truth Table for $f_1^{*}$\relax }}{20}{table.caption.18}\protected@file@percent }
\newlabel{table:truth_table_f_1}{{2.1}{20}{Truth Table for $f_1^{*}$\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Representation of Truth Table for function $f_1^{*}$\relax }}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:f1_XOR}{{2.9}{20}{Representation of Truth Table for function $f_1^{*}$\relax }{figure.caption.19}{}}
\newlabel{eq: weight_f_1}{{2.51}{20}{Feedforward Network}{equation.2.2.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Truth Table for $f_2^{*}$\relax }}{21}{table.caption.20}\protected@file@percent }
\newlabel{table:truth_table_f_2}{{2.2}{21}{Truth Table for $f_2^{*}$\relax }{table.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Representation of Truth Table for function $f_2^{*}$\relax }}{21}{figure.caption.21}\protected@file@percent }
\newlabel{fig:f2_XOR}{{2.10}{21}{Representation of Truth Table for function $f_2^{*}$\relax }{figure.caption.21}{}}
\newlabel{eq: weight_f_2}{{2.53}{21}{Feedforward Network}{equation.2.2.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Truth Table for $f^{*}$\relax }}{22}{table.caption.22}\protected@file@percent }
\newlabel{table:truth_table_f*}{{2.3}{22}{Truth Table for $f^{*}$\relax }{table.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces FFN hidden layer\relax }}{23}{figure.caption.23}\protected@file@percent }
\newlabel{fig:hidden_layer_only}{{2.11}{23}{FFN hidden layer\relax }{figure.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Truth Table from hidden layer\relax }}{23}{table.caption.24}\protected@file@percent }
\newlabel{table:truth_table_hidden_layer}{{2.4}{23}{Truth Table from hidden layer\relax }{table.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Representation of the hidden layer\relax }}{24}{figure.caption.25}\protected@file@percent }
\newlabel{fig:XOR layer}{{2.12}{24}{Representation of the hidden layer\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Feedforward network for XOR problem\relax }}{25}{figure.caption.26}\protected@file@percent }
\newlabel{fig: ffn_XOR_final}{{2.13}{25}{Feedforward network for XOR problem\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Function}{26}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Backpropagation}{28}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{28}{figure.caption.27}\protected@file@percent }
\newlabel{fig:bp_nn_complete}{{2.14}{28}{!!!TO BE CHANGED!!!\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{29}{figure.caption.28}\protected@file@percent }
\newlabel{fig:bp_nn_complete_cons_layer}{{2.15}{29}{!!!TO BE CHANGED!!!\relax }{figure.caption.28}{}}
\newlabel{eq:activation_function_at_l+1 layer}{{2.66}{29}{Backpropagation}{equation.2.4.66}{}}
\newlabel{eq:input of activation_function_at_l+1 layer}{{2.67}{29}{Backpropagation}{equation.2.4.67}{}}
\newlabel{eq:cost_function_definition}{{2.68}{30}{Backpropagation}{equation.2.4.68}{}}
\newlabel{eq:BP_EQ_VEC_1}{{2.69}{30}{Backpropagation}{equation.2.4.69}{}}
\newlabel{eq: change_at_layer_l}{{2.70}{31}{Backpropagation}{equation.2.4.70}{}}
\newlabel{eq: delta_of_cost_bet_layers}{{2.71}{31}{Backpropagation}{equation.2.4.71}{}}
\newlabel{eq: z_j partials}{{2.72}{31}{Backpropagation}{equation.2.4.72}{}}
\newlabel{eq:BP_EQ_VEC_2}{{2.73}{31}{Backpropagation}{equation.2.4.73}{}}
\newlabel{eq:BP_EQ_VEC_3}{{2.74}{32}{Backpropagation}{equation.2.4.74}{}}
\newlabel{eq: partial_C_with_weight}{{2.75}{32}{Backpropagation}{equation.2.4.75}{}}
\newlabel{eq: partial_act_weight}{{2.76}{32}{Backpropagation}{equation.2.4.76}{}}
\newlabel{eq:BP_EQ_VEC_4}{{2.77}{33}{Backpropagation}{equation.2.4.77}{}}
\citation{Rumelhart:1986aa}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Back-Propagation Algorithm\relax }}{34}{algorithm.2}\protected@file@percent }
\newlabel{alg:back_propagation_algo}{{2}{34}{Back-Propagation Algorithm\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Sequential Neural Networks}{34}{section.2.5}\protected@file@percent }
\newlabel{eq:RNN_HL}{{2.86}{34}{Recurrent Neural Network(RNN)}{equation.2.5.86}{}}
\citation{Bengio1994}
\citation{Kolen2001}
\citation{Hochreiter1997}
\newlabel{eq:RNN_OL}{{2.87}{35}{Recurrent Neural Network(RNN)}{equation.2.5.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{35}{figure.caption.30}\protected@file@percent }
\newlabel{}{{2.16}{35}{!!!TO BE CHANGED!!!\relax }{figure.caption.30}{}}
\newlabel{eq:LSTM_HS}{{2.88}{36}{Long Short Term Memory(LSTM)}{equation.2.5.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{36}{figure.caption.32}\protected@file@percent }
\newlabel{LSTM_LAYER}{{2.17}{36}{!!!TO BE CHANGED!!!\relax }{figure.caption.32}{}}
\citation{Schurster1997}
\newlabel{eq:BRNN_LAYER}{{2.92}{37}{Bidirectional RNN(BRNN)}{equation.2.5.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces !!!TO BE CHANGED!!!\relax }}{38}{figure.caption.34}\protected@file@percent }
\newlabel{BRNN_LAYER}{{2.18}{38}{!!!TO BE CHANGED!!!\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Optimization}{39}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:optimization_problem}{{3.1}{39}{Optimization}{equation.3.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces !!!TO BE CHANGED!!! Min max \relax }}{40}{figure.caption.36}\protected@file@percent }
\newlabel{fig:min_max_illustration}{{3.1}{40}{!!!TO BE CHANGED!!! Min max \relax }{figure.caption.36}{}}
\newlabel{eq:grad_f_x}{{3.2}{40}{Determining Minimum/Maximum Point}{equation.3.0.2}{}}
\newlabel{eq:Hessian_matrix}{{3.3}{40}{Determining Minimum/Maximum Point}{equation.3.0.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Optimization algorithms}{41}{section.3.1}\protected@file@percent }
\newlabel{network_error}{{3.4}{41}{Optimization algorithms}{equation.3.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Network Error over different weight\relax }}{41}{figure.caption.38}\protected@file@percent }
\newlabel{network_error_graph}{{3.2}{41}{Network Error over different weight\relax }{figure.caption.38}{}}
\newlabel{derivative_network_error}{{3.5}{42}{Gradient Descent}{equation.3.1.5}{}}
\newlabel{eq:SGM_def}{{3.8}{43}{Stochastic Gradient Descent}{equation.3.1.8}{}}
\newlabel{eq: learning_rates}{{3.9}{43}{Stochastic Gradient Descent}{equation.3.1.9}{}}
\newlabel{mb-sgd}{{3.12}{44}{Mini-Batch Stochastic Gradient Descent}{equation.3.1.12}{}}
\newlabel{eq:ewma_def}{{3.18}{45}{Exponentially Weighted Moving Average}{equation.3.1.18}{}}
\newlabel{eq: SGF_with_momentum}{{3.19}{46}{SGD with Momentum}{equation.3.1.19}{}}
\newlabel{eq:RMSProp_def}{{3.20}{46}{RMSProp}{equation.3.1.20}{}}
\newlabel{eq:Adam_def}{{3.21}{47}{Adam}{equation.3.1.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation using Adam}{47}{section.3.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Backpropagation using Adam optimization with $n$ total number of inputs for $N$ epochs\relax }}{48}{algorithm.3}\protected@file@percent }
\newlabel{alg:back_propagation_adam_algo}{{3}{48}{Backpropagation using Adam optimization with $n$ total number of inputs for $N$ epochs\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Sentiment Analysis}{49}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Rule-Based Methods}{50}{section.4.1}\protected@file@percent }
\citation{Vaswani2017}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Word Embedding}{52}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Transformer Model (Attention Mechanism)}{53}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces !!!TO BE CHANGED!!!-Transformer Layer\relax }}{53}{figure.caption.49}\protected@file@percent }
\newlabel{TRANSFORMER_LAYER}{{4.1}{53}{!!!TO BE CHANGED!!!-Transformer Layer\relax }{figure.caption.49}{}}
\citation{Jacob2018}
\citation{Vaswani2017}
\citation{Chiorrini2021}
\citation{Yinhan2019}
\citation{Yinhan2019}
\bibdata{REFERENCES}
\harvardcite{Bengio1994}{Bengio, Simard \harvardand \^^MFrasconi}{Bengio et~al.}{1994}
\harvardcite{Chiorrini2021}{Chiorrini, Diamantini, Mircoli \harvardand \^^MPotena}{Chiorrini et~al.}{2021}
\harvardcite{Jacob2018}{Devlin, Chang, Lee \harvardand \^^MToutanova}{Devlin et~al.}{2018}
\harvardcite{Hochreiter1997}{Hochreiter \harvardand \ Schmidhuber}{Hochreiter \harvardand \ Schmidhuber}{1997}
\harvardcite{Kolen2001}{Kolen \harvardand \ Kremer}{Kolen \harvardand \ Kremer}{2001}
\harvardcite{LeCun:2015aa}{LeCun, Bengio \harvardand \^^MHinton}{LeCun et~al.}{2015}
\harvardcite{Yinhan2019}{Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer \harvardand \ Stoyanov}{Liu et~al.}{2019}
\harvardcite{McCulloch:1943aa}{McCulloch \harvardand \ Pitts}{McCulloch \harvardand \ Pitts}{1943}
\harvardcite{Rosenblatt:1958aa}{Rosenblatt}{Rosenblatt}{1958}
\harvardcite{Rumelhart:1986aa}{Rumelhart, Hinton \harvardand \^^MWilliams}{Rumelhart et~al.}{1986}
\harvardcite{Schurster1997}{Schuster \harvardand \ Paliwal}{Schuster \harvardand \ Paliwal}{1997}
\harvardcite{Vaswani2017}{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser \harvardand \ Polosukhin}{Vaswani et~al.}{2017}
\gdef \@abspage@last{63}
