%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Varun Parbhu at 2022-11-28 00:49:12 +0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{Hochreiter1997,
	abstract = {{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}},
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	date-added = {2022-11-28 00:48:49 +0400},
	date-modified = {2022-11-28 00:49:05 +0400},
	doi = {10.1162/neco.1997.9.8.1735},
	eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
	issn = {0899-7667},
	journal = {Neural Computation},
	month = {11},
	number = {8},
	pages = {1735-1780},
	title = {{Long Short-Term Memory}},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	volume = {9},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1162/neco.1997.9.8.1735}}

@inbook{Kolen2001,
	author = {Kolen, John F. and Kremer, Stefan C.},
	booktitle = {A Field Guide to Dynamical Recurrent Networks},
	date-added = {2022-11-28 00:43:14 +0400},
	date-modified = {2022-11-28 00:43:24 +0400},
	doi = {10.1109/9780470544037.ch14},
	pages = {237-243},
	title = {Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/9780470544037.ch14}}

@article{Bengio1994,
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	date-added = {2022-11-28 00:39:59 +0400},
	date-modified = {2022-11-28 00:40:19 +0400},
	doi = {10.1109/72.279181},
	journal = {IEEE Transactions on Neural Networks},
	number = {2},
	pages = {157-166},
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1109/72.279181}}

@article{Rosenblatt:1958aa,
	author = {Rosenblatt, F},
	crdt = {1958/11/01 00:00},
	date = {1958 Nov},
	date-added = {2022-11-17 22:22:30 +0400},
	date-modified = {2022-11-17 22:25:31 +0400},
	dcom = {20000701},
	doi = {10.1037/h0042519},
	edat = {1958/11/01 00:00},
	issn = {0033-295X (Print); 0033-295X (Linking)},
	jid = {0376476},
	journal = {Psychol Rev},
	jt = {Psychological review},
	language = {eng},
	lr = {20220408},
	mh = {*Brain; Humans; *Information Storage and Retrieval; *Models, Statistical; *Neural Networks, Computer; *Perception},
	mhda = {1958/11/01 00:01},
	month = {Nov},
	number = {6},
	oid = {CLML: 5935:31561:442},
	oto = {NLM},
	own = {NLM},
	pages = {386--408},
	phst = {1958/11/01 00:00 {$[$}pubmed{$]$}; 1958/11/01 00:01 {$[$}medline{$]$}; 1958/11/01 00:00 {$[$}entrez{$]$}},
	pl = {United States},
	pmid = {13602029},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {OM},
	status = {MEDLINE},
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	volume = {65},
	year = {1958},
	bdsk-url-1 = {https://doi.org/10.1037/h0042519}}

@article{Rumelhart:1986aa,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986/10/01},
	date-added = {2022-11-17 00:46:56 +0400},
	date-modified = {2022-11-17 22:24:34 +0400},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	title = {Learning representations by back-propagating errors},
	volume = {323},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1038/323533a0}}

@article{McCulloch:1943aa,
	abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	author = {McCulloch, Warren S. and Pitts, Walter},
	date = {1943/12/01},
	date-added = {2022-11-17 00:30:15 +0400},
	date-modified = {2022-11-17 22:24:42 +0400},
	doi = {10.1007/BF02478259},
	id = {McCulloch1943},
	isbn = {1522-9602},
	journal = {The bulletin of mathematical biophysics},
	number = {4},
	pages = {115--133},
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	year = {1943},
	bdsk-url-1 = {https://doi.org/10.1007/BF02478259}}

@article{LeCun:2015aa,
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	date = {2015/05/01},
	date-added = {2022-11-17 00:29:17 +0400},
	date-modified = {2022-11-17 22:24:48 +0400},
	doi = {10.1038/nature14539},
	id = {LeCun2015},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7553},
	pages = {436--444},
	title = {Deep learning},
	volume = {521},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1038/nature14539}}
