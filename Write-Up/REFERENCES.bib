%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Varun Parbhu at 2022-11-17 22:25:33 +0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{Rosenblatt:1958aa,
	author = {Rosenblatt, F},
	crdt = {1958/11/01 00:00},
	date = {1958 Nov},
	date-added = {2022-11-17 22:22:30 +0400},
	date-modified = {2022-11-17 22:25:31 +0400},
	dcom = {20000701},
	doi = {10.1037/h0042519},
	edat = {1958/11/01 00:00},
	issn = {0033-295X (Print); 0033-295X (Linking)},
	jid = {0376476},
	journal = {Psychol Rev},
	jt = {Psychological review},
	language = {eng},
	lr = {20220408},
	mh = {*Brain; Humans; *Information Storage and Retrieval; *Models, Statistical; *Neural Networks, Computer; *Perception},
	mhda = {1958/11/01 00:01},
	month = {Nov},
	number = {6},
	oid = {CLML: 5935:31561:442},
	oto = {NLM},
	own = {NLM},
	pages = {386--408},
	phst = {1958/11/01 00:00 {$[$}pubmed{$]$}; 1958/11/01 00:01 {$[$}medline{$]$}; 1958/11/01 00:00 {$[$}entrez{$]$}},
	pl = {United States},
	pmid = {13602029},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {OM},
	status = {MEDLINE},
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	volume = {65},
	year = {1958},
	bdsk-url-1 = {https://doi.org/10.1037/h0042519}}

@article{Rumelhart:1986aa,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986/10/01},
	date-added = {2022-11-17 00:46:56 +0400},
	date-modified = {2022-11-17 22:24:34 +0400},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	title = {Learning representations by back-propagating errors},
	volume = {323},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1038/323533a0}}

@article{McCulloch:1943aa,
	abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	author = {McCulloch, Warren S. and Pitts, Walter},
	date = {1943/12/01},
	date-added = {2022-11-17 00:30:15 +0400},
	date-modified = {2022-11-17 22:24:42 +0400},
	doi = {10.1007/BF02478259},
	id = {McCulloch1943},
	isbn = {1522-9602},
	journal = {The bulletin of mathematical biophysics},
	number = {4},
	pages = {115--133},
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	year = {1943},
	bdsk-url-1 = {https://doi.org/10.1007/BF02478259}}

@article{LeCun:2015aa,
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	date = {2015/05/01},
	date-added = {2022-11-17 00:29:17 +0400},
	date-modified = {2022-11-17 22:24:48 +0400},
	doi = {10.1038/nature14539},
	id = {LeCun2015},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7553},
	pages = {436--444},
	title = {Deep learning},
	volume = {521},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1038/nature14539}}
